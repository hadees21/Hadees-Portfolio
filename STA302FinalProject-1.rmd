---
title: "NBA regular season analysis"
author: "Hadees Khokhar and Ananya Jha"
date: "05/08/2020"
output: pdf_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(cowplot)
```
**INTRODUCTION**

Our inspiration for this project comes from the true story of the book Moneyball by Micheal Lewis. It discusses how sports analytics changed baseball through the story of Oakland A’s, a team near San Francisco, California. We picked Basketball (NBA) as our sport and we will attempt to relate key elements of the sport along with new strategies to the data that we have constructed.

We aim to perform sports analytics and construct a model that relates our predictor variables to our response variable, which is the number of wins of each NBA team during the 2018-19, 2017-18 and 2016-17 seasons. In total we have 90 teams that we will be looking at. We have chosen 6 predictor variables that we expect to impact our response variable and over a series of tests we’ll decide how many of these are relevant and important. We will filter out the redundant information to avoid overfitting our model and analyze which strategies work well for the leading teams. 


```{r, echo=FALSE}
STA302_final_project_data = read.csv('STA302.csv', header=TRUE)
attach(STA302_final_project_data)

```
**METHOD**

The National Basketball Association (NBA) consists of 30 teams across North America. The regular season consists of 82 games, 41 at home and 41 away. Points in basketball can be made two ways - field goals (two or three pointers) and free throws (one point). There are many factors that affect the players and team’s performance in the NBA. As mentioned we will be looking at the 2018-2019, 2017-2018 and 2016-2017 seasons of the NBA, consisting of 90 teams in total.

**HYPOTHESIS TESTING**

We want to prove team chemistry and experience are the most important aspects within a team when it
comes to successful performance in the NBA as teammates must trust each other, get everyone involved, know how to win close games, and have the maturity to make the right judgement calls, thus creating intangibles that a less successful team does not have. We will elaborate how turnovers, assists, and all star players will explain our determine the strength of our hypothesis.

First, we will look at whether or not our data is normally distributed. Then, in order to test the following parameters, we will be using ggplots, residual plots, and QQ plots initially. We will not only look at the correlation between our parameters and the total number of wins but we will also look for correlation between our parameters. After looking at our initial regression line, we will filter out the parameters that are insignificant to our model. Once we’ve removed the insignificant parameters, we will do out model fitting using the significant parameters. 

We have chosen the following parameters to analyze for our data analysis project: \
1)Average height of roster \
2)Average age of roster \
3)Number of All star players on team \
4)Average number of rebounds per game \
5)Average assists per game \
6)Average turnovers per game \

Over our introduction, we want to explain why we chose to analyze each parameter and why we think that it will impact the teams total number of wins. We will also explain how we think they are related to each other (if at all) and whether or not they are redundant. 

1. The first parameter that we will discuss is the average height of a roster. We assume that the taller the team, the better they will perform. This is because, it is common to assume that the taller the players, the more shots blocked, balls stolen, and rebounds attained.
2. Our second parameter is average age. Although older players tend to be not as energetic and are slow to recover, they have the experience to know the right plays, the maturity, emotional intelligence, and skills sharpened that younger players do not have. We expect wins to decline after a certain age as older players can’t keep up with the heavy demand of the regular season.
3. We believe that the number of all stars is a great indicator of wins attained during the season. It goes without saying that better players means a better team but we want to determine whether a team needs a great player to lead it to more wins. We intend to show there is a high multicollinearity with average NBA age as it tends to take time to reach an all star level.
4. We believe average rebounds per game will have a strong relationship with games won as rebounds mean more 2nd chance attempts and stopping opposing teams from 2nd chance attempts. We have hypothesized that there will be a high multicollinearity between average height and total rebounds per game.
5. Teams that share the ball get players in rhythm, boost morale as everyone feels involved, and get open shots. We believe assists per game has a strong impact on the amount of games a team wins.
6. Turnovers are when a team gives up possession of the ball by throwing it out of bounds or letting the other team steal it during play. Turning the ball over means giving up an opportunity to score and giving one to the opposing team. Due to this, we believe it will have a negative correlation with wins. 

\newpage

**SCATTER PLOTS OF PREDICTOR VARIABLES AGAINST RESPONSE VARIABLE**
```{r, echo=FALSE}
sp1<-ggplot(STA302_final_project_data, aes(x = Average_age, y = Wins)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")

sp2<-ggplot(STA302_final_project_data, aes(x = Average_Height, y = Wins)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")

sp3<-ggplot(STA302_final_project_data, aes(x = All_star_players, y = Wins)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")

sp4<-ggplot(STA302_final_project_data, aes(x = Avg_number_of_rebounds, y = Wins)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")
  
  
sp5<-ggplot(STA302_final_project_data, aes(x = Avg_assists_per_game, y = Wins)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")
  
  
sp6<-ggplot(STA302_final_project_data, aes(x = Avg_turnovers_per_game, y = Wins)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")

plot_grid(sp1, sp2, sp3, sp4, sp5, sp6, labels = c("A", "B", "C", "D", "E", "F"))
```
**(A) WINS VS AVERAGE AGE**
Looking at the relationship between the total number of wins per team and and the average age of players on the team, there seems to be a fairly linear positive correlation. Although our line of best fit shows that there are many outliers in the plot, there still seems to be a positive relationship between the two. 

**(B) WINS VS AVERAGE ROSTER HEIGHT** 
In our assumption, we stated that there would be a strong positive relationship between the average height of a team’s roster and the number of wins that the team has. However, our plot shows that there is actually little to no correlation between average team height and number of wins. This contradicts our previous assumption and shows us that height does not really have any effect on how well a team does in a season. This is likely because shorter players are better shooters and taller players are better blockers thus, making it fairly even in terms of performance. 

**(C) WINS VS ALL STAR PLAYERS**
Here there is a very obvious positive relationship between the total number of wins per team and the number of all star players that there are on a team, as we had assumed. This shows us that teams with better players just do better. 


**(D) WINS VS  AVERAGE REBOUNDS PER GAME** 
Once again we had assumed that the average number of rebounds per team would have a positive relationship with the number of wins and that is evident in the plot. Although there are a few outliers as with our other regression lines, there still is a positive relationship and it looks pretty significant.  

**(E) WINS VS AVERAGE ASSISTS PER GAME** 
Again, there seems to be a fairly strong positive linear relationship between wins and assists as the regression line has a relatively steep curve with outliers not lying away too far. This is what we had assumed as we believe a team that has strong chemistry and coordination will all in all perform better as well.  

**(F) WINS VS AVERAGE TURNOVERS PER GAME** 
There seems to be a negative relationship between total number of wins and average turnovers as we had assumed. There are a cluster of outliers in the middle but the points at the ends fit the regression line well. This makes sense as a team with more turnovers has given an opposing team more chances of scoring, and themselves have a smaller chance of scoring. This is evident in the plot. 

\newpage

**A SUMMARY TABLE OF OUR PRELIMINARY MODEL**
```{r, echo=FALSE}


multi.fit = lm(Wins~All_star_players+ Average_age+ Average_Height + Avg_number_of_rebounds + Avg_assists_per_game + Avg_turnovers_per_game, data=STA302_final_project_data)
summary(multi.fit)

```
This multilinear regression consists of all parameters we want to analyze. We can see strong relation and significance but there is still much testing left to be done. 
 
\newpage

```{r, echo=FALSE}
par(mfrow = c(2,2))
multi.res = resid(multi.fit)
fitted=fitted(multi.fit)
plot(STA302_final_project_data$Average_age , multi.res, 
     ylab="Residuals", xlab=" Average age", 
     main="Residual plot") 
abline(0, 0)
hist(STA302_final_project_data$Wins,  breaks = 5)
plot(STA302_final_project_data$Average_Height, multi.res, 
     ylab="Residuals", xlab=" Average Height", 
     main="Residual plot") 
abline(0, 0)
plot(STA302_final_project_data$All_star_players, multi.res, 
     ylab="Residuals", xlab=" Number of all star players", 
     main="Residual plot") 
abline(0, 0)

```
```{r, echo=FALSE}

par(mfrow = c(1,2))
plot(STA302_final_project_data$Avg_assists_per_game, multi.res, 
     ylab="Residuals", xlab=" Average Number of Assists", 
     main="Residual plot") 
abline(0, 0)
plot(STA302_final_project_data$Avg_turnovers_per_game, multi.res, 
     ylab="Residuals", xlab=" Average turnovers per game", 
     main="Residual plot") 
abline(0, 0)
```

**RESIDUAL PLOT ANALYSES**

1. Our residual plot shows that it is normally distributed and our values seem to be equally spaced out around the horizontal axis.
2. The residual plot for average height shows that is normally distributed as they seem to be equally distributed. It seems to be tighter for larger values but it is a negligible amount and can be left to the fact that the sample size is small.
3. Once again, the average number of rebounds also looks to be normally distributed. Rebounds also looks to have an expected value of 0 for errors.
4. Number of all star players is the one variable that looks skewed as it seems to be more accurate of a prediction for a team with more than 1 all star player. This is portraying a fanning effect. 
5. Assists per game also seems to have a very slight fanning effect with it being easier to predict a team’s wins as they win more games. The expected value of the residual plot does appear to be 0 as there seems to be the same amount of values above and below 0.
6. The average turnovers per game also looks like it is normally distributed with an expected value of 0 as well. 

\
\
\

**Assumption 3**

```{r, echo=FALSE}
plot(fitted, multi.res, 
     ylab="Residuals", xlab=" fitted values", 
     main="Residual plot") 
abline(0, 0)  

```
\
We plotted a scatter plot of residuals versus fits. In the plot, it is evident that the variance of our residuals is the same across all values of the x-axis. We notice no patterns (bowtie or megaphones etc) here and so we can safely assume that our variances are consistent (no presence of heteroscedasticity) and assumption 3 is not violated. 

\newpage

**Assumption 4**

```{r, echo=FALSE}
multi.stdres = rstandard(multi.fit)
qqnorm(multi.stdres, 
       ylab="Standardized Residuals", 
       xlab="Normal Scores", 
       main="Normal Q-Q plot") 
qqline(multi.stdres)
hist(multi.stdres)
```
\
For checking the fourth assumption, which is the normality of  errors, we use a normal Q-Q plot. We notice that the data is very slightly left skewed as most of the pattern follows the line (i.e. observations are near the line). So, we can assume the normality of errors. As an additional measure, we plotted the histogram of multi.stdres and noticed the same slight skew. 

\newpage
**CORRELATION BETWEEN PARAMETERS**
```{r, echo=FALSE}

my_data <- STA302_final_project_data[, c(2,3,4,6,7,8)]

res <- cor(my_data)
round(res, 3)

```


The correlation between our parameters seems to be low for the most part. The highest correlation that we can see is between the number of all star players on the team and the average number of assists. Although the correlation between all star players on the team and average number of assists is still low, this is the highest correlation that we can see between our parameters. The next highest correlation we can see on our table is between the number of all star players and the average age of players on the team. However, both of them are insignificant.   
\newpage
**PAIRPLOT**
```{r, echo=FALSE}
pairs(~ Average_Height +  Average_age + Avg_number_of_rebounds + All_star_players +Avg_assists_per_game + Avg_turnovers_per_game, data = STA302_final_project_data)
```




Our pairplot is true to our correlation analysis as there is not much to take away about the relation between each parameter. We can conclude from this as well that there is very little to no multicollinearity. 



```{r, echo=FALSE}

AverageHeight_sum = sum(Average_Height)
AverageHeight_square = sum(Average_Height^2)
Age_sum = sum(Average_age)
Age_square = sum(Average_age^2)
AllStars_sum = sum(All_star_players)
AllStars_square = sum(All_star_players^2)
Assits_sum = sum(Avg_assists_per_game)
Assists_square = sum(Avg_assists_per_game^2)
Turnovers_sum = sum(Avg_turnovers_per_game)
Turnovers_squared = sum(Avg_turnovers_per_game^2)
Rebounds_sum = sum(Avg_number_of_rebounds)
Rebounds_squared = sum(Avg_number_of_rebounds^2)


Wins_sum = sum(Wins)
AverageHeight_Wins = sum(Average_Height*Wins)
Age_Wins = sum(Average_age*Wins)
AllStars_Wins = sum(All_star_players*Wins)
Assists_Wins = sum(Avg_assists_per_game*Wins)
Turnovers_Wins = sum(Avg_turnovers_per_game*Wins)
Rebounds_Wins = sum(Avg_number_of_rebounds)

Height_Age = sum(Average_Height*Average_age)
Height_Rebounds = sum(Average_Height*Avg_number_of_rebounds)
Height_Experience = sum(Average_Height*Average_age)
Height_AllStars = sum(Average_Height*All_star_players)
Height_Assists = sum(Average_Height*Avg_assists_per_game)
Height_Turnovers = sum(Average_Height*Avg_turnovers_per_game)

Age_AllStars = sum(Average_age*All_star_players)
Age_Assists = sum(Average_age*Avg_assists_per_game)
Age_Rebounds = sum(Average_age*Avg_number_of_rebounds)
Age_Turnovers = sum(Average_age*Avg_turnovers_per_game)

AllStars_assists = sum(All_star_players*Avg_assists_per_game)
AllStars_turnovers = sum(All_star_players*Avg_turnovers_per_game)
AllStars_Rebounds = sum(All_star_players*Avg_number_of_rebounds)

Assists_turnovers = sum(Avg_turnovers_per_game*Avg_assists_per_game)
Assists_Rebounds = sum(Avg_assists_per_game*Avg_number_of_rebounds)

Turnovers_Rebounds = sum(Avg_turnovers_per_game*Avg_number_of_rebounds)

I = rep(1,90)
X=cbind(I, Average_Height, Avg_number_of_rebounds  , Average_age, All_star_players, Avg_assists_per_game, Avg_turnovers_per_game)
Y=cbind(STA302_final_project_data$Wins)
XY=t(X)%*%Y
XX=t(X)%*%X
Inverse=solve(XX)
beta=Inverse%*%XY

multi.fit = lm(Wins ~ Average_Height + Avg_number_of_rebounds+ Average_age + All_star_players + Avg_assists_per_game + Avg_turnovers_per_game, data=STA302_final_project_data)
summary(multi.fit)
```
\newpage

```{r, echo=FALSE, fig.keep = "none", results = "hide"}

n <- length(Wins)

lin.fit = lm(Wins~Average_Height, data=STA302_final_project_data)
summary(lin.fit)
SS.lin.fit = sum(resid(lin.fit)^2)
p_prime.lin.fit <- length(lin.fit$coefficients)
AIC.lin.fit <- n*log(SS.lin.fit) - n*log(n) + 2*p_prime.lin.fit

lin.fit2 = lm(Wins~Average_age, data=STA302_final_project_data)
summary(lin.fit2)

lin.fit3 = lm(Wins~All_star_players, data=STA302_final_project_data)
summary(lin.fit3)
SS.lin.fit3 = sum(resid(lin.fit3)^2)
p_prime.lin.fit3 <- length(lin.fit3$coefficients)
AIC.lin.fit3 <- n*log(SS.lin.fit3) - n*log(n) + 2*p_prime.lin.fit3

lin.fit4 = lm(Wins~Avg_assists_per_game, data=STA302_final_project_data)
summary(lin.fit4)
SS.lin.fit4 = sum(resid(lin.fit4)^2)
p_prime.lin.fit4 <- length(lin.fit4$coefficients)
AIC.lin.fit4 <- n*log(SS.lin.fit4) - n*log(n) + 2*p_prime.lin.fit4



lin.fit5 = lm(Wins~Avg_turnovers_per_game, data=STA302_final_project_data)
summary(lin.fit5)
poly.fit1 = lm(Wins~I(All_star_players^2), data=STA302_final_project_data)
summary(poly.fit1)
poly.fit2 = lm(Wins~I(Average_age^2), data=STA302_final_project_data)
summary(poly.fit2)
poly.fit3 = lm(Wins~I(Average_Height^2), data=STA302_final_project_data)
summary(poly.fit3)

poly.fit4 = lm(Wins~I(Avg_assists_per_game^2), data=STA302_final_project_data)
summary(poly.fit4)
SS.poly.fit4 = sum(resid(poly.fit4)^2)
p_prime.poly.fit4 <- length(poly.fit4$coefficients)
AIC.poly.fit4 <- n*log(SS.poly.fit4) - n*log(n) + 2*p_prime.poly.fit4

poly.fit5 = lm(Wins~I(Avg_turnovers_per_game^2), data=STA302_final_project_data)
summary(poly.fit5)



poly.fit6 = lm(Wins~ All_star_players+ I(All_star_players^2)+ I(All_star_players  * All_star_players^2), data=STA302_final_project_data)
summary(poly.fit6)

poly.fit7 = lm(Wins~Average_age  + I(Average_age^2) + I(Average_age * Average_age^2), data=STA302_final_project_data)
summary(poly.fit7)
SS.poly.fit7 = sum(resid(poly.fit7)^2)
p_prime.poly.fit7 <- length(poly.fit7$coefficients)
AIC.poly.fit7 <- n*log(SS.poly.fit7) - n*log(n) + 2*p_prime.poly.fit7


poly.fit8 = lm(Wins~Avg_assists_per_game + I(Avg_assists_per_game^2) + I(Avg_assists_per_game * Avg_assists_per_game^2), data=STA302_final_project_data)
summary(poly.fit8)
SS.poly.fit8 = sum(resid(poly.fit8)^2)
p_prime.poly.fit8 <- length(poly.fit8$coefficients)
AIC.poly.fit8 <- n*log(SS.poly.fit8) - n*log(n) + 2*p_prime.poly.fit8

poly.fit9 = lm(Wins~ Avg_turnovers_per_game+ I(Avg_turnovers_per_game^2)+ I(Avg_turnovers_per_game  * Avg_turnovers_per_game^2), data=STA302_final_project_data)
summary(poly.fit9)




multilin.fit1= lm(Wins~All_star_players+Avg_assists_per_game + I(All_star_players  * Avg_assists_per_game), data=STA302_final_project_data)
summary(multilin.fit1)
SS.multilin.fit1 = sum(resid(multilin.fit1)^2)
p_prime.multilin.fit1 <- length(multilin.fit1$coefficients)
AIC.multilin.fit1 <- n*log(SS.multilin.fit1) - n*log(n) + 2*p_prime.multilin.fit1

multilin.fit2= lm(Wins~Avg_turnovers_per_game+Avg_assists_per_game, data=STA302_final_project_data)
summary(multilin.fit2)
SS.multilin.fit2 = sum(resid(multilin.fit2)^2)
p_prime.multilin.fit2 <- length(multilin.fit2$coefficients)
AIC.multilin.fit2 <- n*log(SS.multilin.fit2) - n*log(n) + 2*p_prime.multilin.fit2


multilin.fit3= lm(Wins~All_star_players+Average_age, data=STA302_final_project_data)
summary(multilin.fit3)

multilin.fit4= lm(Wins~Avg_assists_per_game+Average_age, data=STA302_final_project_data)
summary(multilin.fit4)
multilin.fit5= lm(Wins~Avg_assists_per_game+Average_age +All_star_players, data=STA302_final_project_data)
summary(multilin.fit5)




Multipolynomial.fit1 = lm(Wins~Avg_assists_per_game + All_star_players +Average_Height + I(Avg_assists_per_game  * All_star_players), data=STA302_final_project_data)
summary(Multipolynomial.fit1)


Multipolynomial.fit2  = lm(Wins~Avg_assists_per_game + All_star_players +Avg_turnovers_per_game + I(Avg_assists_per_game  * Average_Height), data=STA302_final_project_data)
summary(Multipolynomial.fit2)
SS.Multipolynomial.fit2 = sum(resid(Multipolynomial.fit2)^2)
p_prime.Multipolynomial.fit2 <- length(Multipolynomial.fit2$coefficients)
AIC.Multipolynomial.fit2 <- n*log(SS.Multipolynomial.fit2) - n*log(n) + 2*p_prime.Multipolynomial.fit2


Multipolynomial.fit3 = lm(Wins~Avg_assists_per_game + All_star_players + I(All_star_players  * Average_Height), data=STA302_final_project_data)
summary(Multipolynomial.fit3)

Multipolynomial.fit4 = lm(Wins~Avg_assists_per_game + All_star_players + Avg_turnovers_per_game + I(Avg_assists_per_game  * Avg_turnovers_per_game), data=STA302_final_project_data)
summary(Multipolynomial.fit4)
SS.Multipolynomial.fit4 = sum(resid(Multipolynomial.fit4)^2)
p_prime.Multipolynomial.fit4 <- length(Multipolynomial.fit4$coefficients)
AIC.Multipolynomial.fit4 <- n*log(SS.Multipolynomial.fit4) - n*log(n) + 2*p_prime.Multipolynomial.fit4


Multipolynomial.fit5  = lm(Wins~Avg_assists_per_game + All_star_players +Avg_turnovers_per_game , data=STA302_final_project_data)
summary(Multipolynomial.fit5)
SS.Multipolynomial.fit5 = sum(resid(Multipolynomial.fit5)^2)
p_prime.Multipolynomial.fit5 <- length(Multipolynomial.fit5$coefficients)
AIC.Multipolynomial.fit5 <- n*log(SS.Multipolynomial.fit5) - n*log(n) + 2*p_prime.Multipolynomial.fit5

```


\newpage

**MODEL SELECTION**

$X_1 =$ Average Age
$X_2 =$ Assists
$X_3 =$ Turnovers
$X_4 =$ Number of All Star Players
$X_5 =$ Rebounds
$X_6 =$ Average Height
```{r, echo=FALSE}
prof=read.csv("table.csv")
print(prof)
```

For our model selection, we tried out multiple models, starting with all of our parameters included. Our main criterion were the SSRes (For $AIC_p$ criterion), T statistics of the predictor variables, R^2 and R^2 adjusted and significance of impact on the model (i.e T values). We proceeded with the backwards selection method.  We started with all parameters and individual parameters to test our personal hypothesis and moved forward from there. If a parameter such as average height was very obviously insignificant, we would conduct less testing on it and drop it on fairly early on.

We had an intuition that Average Height was going to be insignificant after the result of our preliminary analysis. However, we picked out one model with an interaction term between average height and number of assists for our table of models to demonstrate that after adding another term, the R^2 adjusted barely increased and so, that term was insignificant. We included interaction terms for those that were slightly correlated as no two parameters were strongly correlated. An example of this is All star players and Average assists per game.

We tried up to the 3rd degree for the more significant parameters such as age but tested very little for higher degrees on parameters such as all stars because it became apparent very early that most of our parameters had a linear relationship with Wins if any at all.

We narrowed it down to 10 combinations of linear parameters, higher degree parameters, and interaction terms. As expected, as we added more predictor variables, $R^2$ increased. We stopped adding predictor variables when we noticed that adding more was leading to an extremely small increase in $R^2$ which was our last model with $R^2 = 0.72$. Our second criterion was $R^2$ adjusted and we picked our model that was the simplest(had linear terms only) and had $R^2=0.7$ which we determined to be the upper limit of all the model's adjusted $R^2$. 

We also calculated the $AIC_p$ criterion for our shortlisted models and picked the one with the smallest that was $AIC_p = 335.850$. 

We believe that we have found an optimal model that truly represents our data. We found other models with higher R squared and adjusted R squared values when we included age and age^2 relating to our hypotheses of teams near an average age of 27 performing better. However, we decided not to include it as it wasnt significant in our model and didn’t fully understand the bias of how teams who averaged close to 27 over a longer range versus teams that were tightly knit to an average of age 27. 
	
The model that we picked after thorough analysis was the 10th model with the predictor variables average number of turnovers, average number of assists and number of all star players, all as linear variables.  
\newpage
This is a summary table of our **Final Model**
```{r}
summary(Multipolynomial.fit5)
```

\newpage
**MODEL VALIDATION**
\
For model validation, we have used our original set of 3 seasons as the training set(season 19', 18', 17') and will be using season 16' as our test set. Our training model only consists of 90 elements and test set of 30. Although the training set is small, we decided to go through with validation as our sets our independent. From our diagnosis plot, we can see the residual vs fitted values is approximately normal with an expected value of 0 and the same applies for the Q-Q plot. From the scale location plot, we can see the line is following a horizontal line but could be improved. This satisfies the assumption of homoscedasticity.From our residual vs leverage plot, we see we have no problematic outliers influencing our data.
Our test model output has a strong R squared and adjusted R squared values fortifying our model. We are content with our selected model

```{r, echo=FALSE}
STA302_test_data = read.csv('STA302testdata.csv', header=TRUE)
attach(STA302_test_data)
training_set = STA302_final_project_data
test_set = STA302_test_data
training_model  = lm(Wins~Avg_assists_per_game + All_star_players +Avg_turnovers_per_game , data=training_set)
summary(training_model)

test_model  = lm(Wins~Avg_assists_per_game + All_star_players +Avg_turnovers_per_game , data=test_set)
summary(test_model)

plot(test_model)

```

\newpage 

**CONCLUSION**
\
In our conclusion, we will explore which of our parameters were of significance, which were not, the final model and other observations that we made throughout this project . Some of our predictions were clearly insignificant, some were shocking and some were very obviously significant as we had earlier assumed. A few were non-linear which we have taken into consideration when adjusting our model. Most of our parameters do not violate our assumptions but they are a lot less significant than we had assumed. All star players, Turnovers, and Assists were the most significant parameters in our model. This is what we had expected as in our hypothesis it is stated that team chemistry is the most important aspect when it comes to a team’s success and turnovers and assists in particular show that a team is playing well together. Average age and average rebounds were of less significance but still significant nonetheless. And, average height was not as significant as we had assumed and so is not included in our final model. 

 We assumed that average age played a big part in the total number of wins as veterans know strategies and techniques better than the rookies and younger players who do not have those skills yet. We also assumed that veterans would provide the leadership that adds to a team’s coercion as it’s something they would’ve gained from many years of playing the sport. We concluded that this was irrelevant for the most part. The same can be said for rebounds as the logical conclusion was that the more rebounds a team got, the more chances to score or stop the opposing team from scoring. This was not the case and the average number of rebounds was not as significant. 

What was shocking was that shorter teams performed slightly better than taller teams. Even though height did not make a significant difference overall, we assumed this might be due to more skilled and shooting focused players being in the range of 6’ and 6’6 and taller players being able to block shots and such. Thus, showing us that height is insignificant and teams with taller players don’t necessarily do better. For this reason, we left average height out of our final model. 

Our model definitely had some limitations. The fact that our population size was only 90 was especially problematic. As we know, outliers affect data greatly when the sample size is smaller. Another limitation was that some teams were purposely trying to throw games as they wanted to “Tank” and secure a high position in the NBA draft. This is when teams pick rookies coming out of college at the end of each season and the teams with lower wins have better odds at the top picks.

What we can relate back to our initial conclusion though is assists, turnovers and all star players which are all components of a cohesive team. This provides more opportunities for everyone to get involved, allows for careful play, less mistakes, and lets great players lead the team. Our final model consists of the following parameters - average assists, average turnovers and number of all star players. This makes sense as we can relate it back to our initial hypothesis about team coercion and all of these parameters affect how well a team moves and plays together. 

We decided this model was optimal as it shows other factors such as rebounds and height were not as important when it came to regular season success.  These other parameters created noise and were insignificant.  Due to some non linear distribution with parameters, we decided to square some to train it better for our model. The other parameters created noise and were insignificant. 

In conclusion, our original hypothesis holds in the sense that all the parameters that were significant add and contribute to a team’s chemistry. All of the assists, all star players, turnovers and even age and rebounds to some extent contribute to how well the team performs as a whole. We believe it is safe to state that basketball and the NBA is definitely not an individual focused sport and that it not only requires contribution from all members of the team but also requires them to move and groove well together. 

To summaraize our analysis, we wanted to make sure our data would truly represent the NBA as a whole and fit for analysis, checked relativity to wins, created a model we thought was truly predictive, and verified its fitting with another test set to conclude that team coehesivness and star power lead to wins in the NBA.

\newpage 

**APPENDIX**
```{r, fig.keep = "none", results = "hide"}
sp1<-ggplot(STA302_final_project_data, aes(x = Average_age, y = Wins)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")

sp2<-ggplot(STA302_final_project_data, aes(x = Average_Height, y = Wins)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")

sp3<-ggplot(STA302_final_project_data, aes(x = All_star_players, y = Wins)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")

sp4<-ggplot(STA302_final_project_data, aes(x = Avg_number_of_rebounds, y = Wins)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")
  
  
sp5<-ggplot(STA302_final_project_data, aes(x = Avg_assists_per_game, y = Wins)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")
  
  
sp6<-ggplot(STA302_final_project_data, aes(x = Avg_turnovers_per_game, y = Wins)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")

plot_grid(sp1, sp2, sp3, sp4, sp5, sp6, labels = c("A", "B", "C", "D", "E", "F"))

multi.fit = lm(Wins~All_star_players+ Average_age+ Average_Height + Avg_number_of_rebounds + Avg_assists_per_game + Avg_turnovers_per_game, data=STA302_final_project_data)
summary(multi.fit)
par(mfrow = c(2,2))
multi.res = resid(multi.fit)
fitted=fitted(multi.fit)
plot(STA302_final_project_data$Average_age , multi.res, 
     ylab="Residuals", xlab=" Average age", 
     main="Residual plot") 
abline(0, 0)
hist(STA302_final_project_data$Wins,  breaks = 5)
plot(STA302_final_project_data$Average_Height, multi.res, 
     ylab="Residuals", xlab=" Average Height", 
     main="Residual plot") 
abline(0, 0)
plot(STA302_final_project_data$All_star_players, multi.res, 
     ylab="Residuals", xlab=" Number of all star players", 
     main="Residual plot") 
abline(0, 0)
par(mfrow = c(1,2))
plot(STA302_final_project_data$Avg_assists_per_game, multi.res, 
     ylab="Residuals", xlab=" Average Number of Assists", 
     main="Residual plot") 
abline(0, 0)
plot(STA302_final_project_data$Avg_turnovers_per_game, multi.res, 
     ylab="Residuals", xlab=" Average turnovers per game", 
     main="Residual plot") 
abline(0, 0)

plot(fitted, multi.res, 
     ylab="Residuals", xlab=" fitted values", 
     main="Residual plot") 
abline(0, 0)  

multi.stdres = rstandard(multi.fit)
qqnorm(multi.stdres, 
       ylab="Standardized Residuals", 
       xlab="Normal Scores", 
       main="Normal Q-Q plot") 
qqline(multi.stdres)
hist(multi.stdres)

my_data <- STA302_final_project_data[, c(2,3,4,6,7,8)]

res <- cor(my_data)
round(res, 3)

pairs(~ Average_Height +  Average_age + Avg_number_of_rebounds + All_star_players +Avg_assists_per_game + Avg_turnovers_per_game, data = STA302_final_project_data)


I = rep(1,90)
X=cbind(I, Average_Height, Avg_number_of_rebounds  , Average_age, All_star_players, Avg_assists_per_game, Avg_turnovers_per_game)
Y=cbind(STA302_final_project_data$Wins)
XY=t(X)%*%Y
XX=t(X)%*%X
Inverse=solve(XX)
beta=Inverse%*%XY

multi.fit = lm(Wins ~ Average_Height + Avg_number_of_rebounds+ Average_age + All_star_players + Avg_assists_per_game + Avg_turnovers_per_game, data=STA302_final_project_data)
summary(multi.fit)
anova(multi.fit)

n <- length(Wins)

lin.fit = lm(Wins~Average_Height, data=STA302_final_project_data)
summary(lin.fit)
SS.lin.fit = sum(resid(lin.fit)^2)
p_prime.lin.fit <- length(lin.fit$coefficients)
AIC.lin.fit <- n*log(SS.lin.fit) - n*log(n) + 2*p_prime.lin.fit

lin.fit2 = lm(Wins~Average_age, data=STA302_final_project_data)
summary(lin.fit2)

lin.fit3 = lm(Wins~All_star_players, data=STA302_final_project_data)
summary(lin.fit3)
SS.lin.fit3 = sum(resid(lin.fit3)^2)
p_prime.lin.fit3 <- length(lin.fit3$coefficients)
AIC.lin.fit3 <- n*log(SS.lin.fit3) - n*log(n) + 2*p_prime.lin.fit3

lin.fit4 = lm(Wins~Avg_assists_per_game, data=STA302_final_project_data)
summary(lin.fit4)
SS.lin.fit4 = sum(resid(lin.fit4)^2)
p_prime.lin.fit4 <- length(lin.fit4$coefficients)
AIC.lin.fit4 <- n*log(SS.lin.fit4) - n*log(n) + 2*p_prime.lin.fit4



lin.fit5 = lm(Wins~Avg_turnovers_per_game, data=STA302_final_project_data)
summary(lin.fit5)
poly.fit1 = lm(Wins~I(All_star_players^2), data=STA302_final_project_data)
summary(poly.fit1)
poly.fit2 = lm(Wins~I(Average_age^2), data=STA302_final_project_data)
summary(poly.fit2)
poly.fit3 = lm(Wins~I(Average_Height^2), data=STA302_final_project_data)
summary(poly.fit3)

poly.fit4 = lm(Wins~I(Avg_assists_per_game^2), data=STA302_final_project_data)
summary(poly.fit4)
SS.poly.fit4 = sum(resid(poly.fit4)^2)
p_prime.poly.fit4 <- length(poly.fit4$coefficients)
AIC.poly.fit4 <- n*log(SS.poly.fit4) - n*log(n) + 2*p_prime.poly.fit4

poly.fit5 = lm(Wins~I(Avg_turnovers_per_game^2), data=STA302_final_project_data)
summary(poly.fit5)



poly.fit6 = lm(Wins~ All_star_players+ I(All_star_players^2)+ I(All_star_players  * All_star_players^2), data=STA302_final_project_data)
summary(poly.fit6)

poly.fit7 = lm(Wins~Average_age  + I(Average_age^2) + I(Average_age * Average_age^2), data=STA302_final_project_data)
summary(poly.fit7)
SS.poly.fit7 = sum(resid(poly.fit7)^2)
p_prime.poly.fit7 <- length(poly.fit7$coefficients)
AIC.poly.fit7 <- n*log(SS.poly.fit7) - n*log(n) + 2*p_prime.poly.fit7


poly.fit8 = lm(Wins~Avg_assists_per_game + I(Avg_assists_per_game^2) + I(Avg_assists_per_game * Avg_assists_per_game^2), data=STA302_final_project_data)
summary(poly.fit8)
SS.poly.fit8 = sum(resid(poly.fit8)^2)
p_prime.poly.fit8 <- length(poly.fit8$coefficients)
AIC.poly.fit8 <- n*log(SS.poly.fit8) - n*log(n) + 2*p_prime.poly.fit8

poly.fit9 = lm(Wins~ Avg_turnovers_per_game+ I(Avg_turnovers_per_game^2)+ I(Avg_turnovers_per_game  * Avg_turnovers_per_game^2), data=STA302_final_project_data)
summary(poly.fit9)




multilin.fit1= lm(Wins~All_star_players+Avg_assists_per_game + I(All_star_players  * Avg_assists_per_game), data=STA302_final_project_data)
summary(multilin.fit1)
SS.multilin.fit1 = sum(resid(multilin.fit1)^2)
p_prime.multilin.fit1 <- length(multilin.fit1$coefficients)
AIC.multilin.fit1 <- n*log(SS.multilin.fit1) - n*log(n) + 2*p_prime.multilin.fit1

multilin.fit2= lm(Wins~Avg_turnovers_per_game+Avg_assists_per_game, data=STA302_final_project_data)
summary(multilin.fit2)
SS.multilin.fit2 = sum(resid(multilin.fit2)^2)
p_prime.multilin.fit2 <- length(multilin.fit2$coefficients)
AIC.multilin.fit2 <- n*log(SS.multilin.fit2) - n*log(n) + 2*p_prime.multilin.fit2


multilin.fit3= lm(Wins~All_star_players+Average_age, data=STA302_final_project_data)
summary(multilin.fit3)

multilin.fit4= lm(Wins~Avg_assists_per_game+Average_age, data=STA302_final_project_data)
summary(multilin.fit4)
multilin.fit5= lm(Wins~Avg_assists_per_game+Average_age +All_star_players, data=STA302_final_project_data)
summary(multilin.fit5)




Multipolynomial.fit1 = lm(Wins~Avg_assists_per_game + All_star_players +Average_Height + I(Avg_assists_per_game  * All_star_players), data=STA302_final_project_data)
summary(Multipolynomial.fit1)


Multipolynomial.fit2  = lm(Wins~Avg_assists_per_game + All_star_players +Avg_turnovers_per_game + I(Avg_assists_per_game  * Average_Height), data=STA302_final_project_data)
summary(Multipolynomial.fit2)
SS.Multipolynomial.fit2 = sum(resid(Multipolynomial.fit2)^2)
p_prime.Multipolynomial.fit2 <- length(Multipolynomial.fit2$coefficients)
AIC.Multipolynomial.fit2 <- n*log(SS.Multipolynomial.fit2) - n*log(n) + 2*p_prime.Multipolynomial.fit2


Multipolynomial.fit3 = lm(Wins~Avg_assists_per_game + All_star_players + I(All_star_players  * Average_Height), data=STA302_final_project_data)
summary(Multipolynomial.fit3)

Multipolynomial.fit4 = lm(Wins~Avg_assists_per_game + All_star_players + Avg_turnovers_per_game + I(Avg_assists_per_game  * Avg_turnovers_per_game), data=STA302_final_project_data)
summary(Multipolynomial.fit4)
SS.Multipolynomial.fit4 = sum(resid(Multipolynomial.fit4)^2)
p_prime.Multipolynomial.fit4 <- length(Multipolynomial.fit4$coefficients)
AIC.Multipolynomial.fit4 <- n*log(SS.Multipolynomial.fit4) - n*log(n) + 2*p_prime.Multipolynomial.fit4


Multipolynomial.fit5  = lm(Wins~Avg_assists_per_game + All_star_players +Avg_turnovers_per_game , data=STA302_final_project_data)
summary(Multipolynomial.fit5)
SS.Multipolynomial.fit5 = sum(resid(Multipolynomial.fit5)^2)
p_prime.Multipolynomial.fit5 <- length(Multipolynomial.fit5$coefficients)
AIC.Multipolynomial.fit5 <- n*log(SS.Multipolynomial.fit5) - n*log(n) + 2*p_prime.Multipolynomial.fit5

prof=read.csv("table.csv")
print(prof)

STA302_test_data = read.csv('STA302testdata.csv', header=TRUE)
attach(STA302_test_data)
training_set = STA302_final_project_data
test_set = STA302_test_data
training_model  = lm(Wins~Avg_assists_per_game + All_star_players +Avg_turnovers_per_game , data=training_set)
summary(training_model)

test_model  = lm(Wins~Avg_assists_per_game + All_star_players +Avg_turnovers_per_game , data=test_set)
summary(test_model)

plot(test_model)
```


**References (For Dataset)**
https://www.basketball-reference.com


```

